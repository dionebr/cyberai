# Guia de Instala√ß√£o - CyberAI

## üîß Requisitos do Sistema

### M√≠nimos (com TinyLlama)
- **OS:** Linux, macOS ou Windows com WSL
- **RAM:** 2GB dispon√≠vel
- **Armazenamento:** 2GB livre
- **CPU:** Qualquer processador moderno

### Recomendados (com sistema h√≠brido)
- **OS:** Linux (Arch, Ubuntu, Debian, etc.)
- **RAM:** 8GB+ (para usar ambos os modelos)
- **Armazenamento:** 10GB livre
- **CPU:** 4+ cores
- **GPU:** Opcional (melhora performance)

---

## üê≥ Instala√ß√£o com Docker (Recomendada)

### M√©todo Mais Simples - Um Comando
```bash
git clone https://github.com/dionebr/cyberai.git
cd cyberai
docker compose up --build -d
```

### Acesso
- **URL:** `http://localhost`
- **API:** `http://localhost/api/models` (para ver modelos dispon√≠veis)

### Comandos √öteis
```bash
# Ver logs em tempo real
docker compose logs -f

# Parar os servi√ßos
docker compose down

# Reiniciar apenas a API
docker compose restart api
```

---

## üíª Instala√ß√£o Manual (Avan√ßada)

### 1. Clone e Setup Inicial
```bash
git clone https://github.com/dionebr/cyberai.git
cd cyberai
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Download dos Modelos

**TinyLlama (R√°pido - 668MB):**
```bash
cd models
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
```

**Mistral (Opcional - Completo - 4GB):**
```bash
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir models --local-dir-use-symlinks False
```

### 3. Iniciar Servi√ßos
```bash
# Terminal 1: API
source .venv/bin/activate
python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000

# Terminal 2: Web Server (em outra aba)
cd src/web && python -m http.server 8080
```

### 4. Acesso
- **URL:** `http://localhost:8080/templates/index.html`

---

## ‚ö° Configura√ß√£o de Performance

### Para M√°quinas Limitadas
O sistema automaticamente usa apenas TinyLlama para todas as tarefas. Para for√ßar isto:

1. Remova ou renomeie o arquivo `mistral-7b-instruct-v0.2.Q4_K_M.gguf`
2. O sistema usar√° apenas TinyLlama (ultra-r√°pido)

### Para M√°quinas Potentes
Mantenha ambos os modelos para aproveitar o sistema h√≠brido inteligente.

### Otimiza√ß√µes de GPU (Opcional)
```bash
# Para NVIDIA GPU
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python

# Para AMD GPU  
CMAKE_ARGS="-DLLAMA_HIPBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python

# Para Apple Silicon
CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python
```

---

## üîß Verifica√ß√£o da Instala√ß√£o

### Teste da API
```bash
curl http://localhost:8000/api/techniques
curl http://localhost:8000/api/models
```

### Teste R√°pido
```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"nmap b√°sico","technique":"recon","max_tokens":50}'
```

---

## üÜò Solu√ß√£o de Problemas

### Erro: "Modelo indispon√≠vel"
- **Causa:** Arquivo GGUF n√£o encontrado
- **Solu√ß√£o:** Verifique se os arquivos est√£o na pasta `models/`

### Erro: Timeout nas respostas
- **Causa:** Modelo muito pesado para a m√°quina
- **Solu√ß√£o:** Use apenas TinyLlama (remova o Mistral)

### Erro: "llama-cpp-python" n√£o funciona
- **Solu√ß√£o:** Reinstale com flags espec√≠ficas da sua arquitetura (ver se√ß√£o GPU acima)

### Interface n√£o carrega
- **Causa:** Servidor web n√£o est√° rodando corretamente
- **Solu√ß√£o:** Verifique se est√° servindo a pasta `src/web/templates`
