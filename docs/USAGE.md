# Guia de Uso - CyberAI

## üß† Sistema H√≠brido de IA

O CyberAI usa **dois modelos de IA** que trabalham juntos para otimizar velocidade e qualidade:

### ‚ö° Sele√ß√£o Autom√°tica Inteligente
- **TinyLlama (668MB):** Tarefas simples = respostas em 5-10 segundos
- **Mistral (4GB):** Tarefas complexas = an√°lises detalhadas
- **Autom√°tico:** O sistema escolhe o modelo ideal baseado na t√©cnica

### üìä Mapeamento de T√©cnicas
**TinyLlama (R√°pido):**
- `recon`, `payloads`, `xss`, `sql_injection`, `buffer_overflow`
- `post`, `binarios`, `exploit`, `relatorios`, `gestao` 
- `ferramentas`, `intelligence`, `lab`, `treinamento`

**Mistral (Completo):**
- T√©cnicas n√£o listadas acima (an√°lises muito complexas)

---

## üê≥ Op√ß√£o A ‚Äî Docker (Recomendado)

### 1. Subir os Servi√ßos
```bash
git clone https://github.com/dionebr/cyberai.git
cd cyberai
docker compose up --build -d
```

### 2. Acessar Interface Web
- **Principal:** http://localhost
- **API Direta:** http://localhost/api/models

### 3. M√≥dulos Dispon√≠veis
| URL | M√≥dulo | Modelo | Velocidade |
|-----|--------|--------|------------|
| `/recon.html` | Reconhecimento | TinyLlama | ‚ö° Ultra-r√°pido |
| `/payloads.html` | Payloads | TinyLlama | ‚ö° Ultra-r√°pido |
| `/binarios.html` | An√°lise Bin√°rios | TinyLlama | ‚ö° Ultra-r√°pido |
| `/exploit.html` | Explora√ß√£o | TinyLlama | ‚ö° Ultra-r√°pido |
| `/post.html` | P√≥s-Explora√ß√£o | TinyLlama | ‚ö° Ultra-r√°pido |
| `/treinamento.html` | Treinamento | TinyLlama | ‚ö° Ultra-r√°pido |
| `/relatorios.html` | Relat√≥rios | TinyLlama | ‚ö° Ultra-r√°pido |
| `/gestao.html` | Gest√£o | TinyLlama | ‚ö° Ultra-r√°pido |
| `/ferramentas.html` | Dev Tools | TinyLlama | ‚ö° Ultra-r√°pido |
| `/intelligence.html` | Threat Intel | TinyLlama | ‚ö° Ultra-r√°pido |
| `/lab.html` | Laborat√≥rio | TinyLlama | ‚ö° Ultra-r√°pido |
| `/community.html` | Comunidade | Autom√°tico | üîÑ H√≠brido |

### 4. Comandos √öteis
```bash
# Ver logs em tempo real
docker compose logs -f

# Reiniciar API (ap√≥s mudan√ßas)
docker compose restart api

# Parar tudo
docker compose down
```

---

## üíª Op√ß√£o B ‚Äî Execu√ß√£o Local

### 1. Setup Completo
```bash
git clone https://github.com/dionebr/cyberai.git
cd cyberai
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Baixar TinyLlama (essencial)
cd models
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
```

### 2. Iniciar Servi√ßos
```bash
# Terminal 1: API
source .venv/bin/activate
python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000

# Terminal 2: Web
cd src/web && python -m http.server 8080
```

### 3. Acesso
- **URL:** http://localhost:8080/templates/index.html
- **API:** http://localhost:8000/api/models

---

## üöÄ Endpoints da API

### Principais
- `POST /api/generate` ‚Äî Gera resposta (sele√ß√£o autom√°tica de modelo)
- `POST /api/generate_exploit` ‚Äî Exploits (usa Mistral para qualidade)
- `GET /api/techniques` ‚Äî Lista t√©cnicas suportadas
- `GET /api/models` ‚Äî Informa√ß√µes dos modelos dispon√≠veis

### Novo: Endpoint de Modelos
```bash
curl http://localhost/api/models
```
**Resposta:**
```json
{
  "models": [
    {
      "name": "TinyLlama (R√°pido)",
      "description": "Modelo r√°pido para tarefas simples (600MB)",
      "techniques": ["recon", "payloads", ...],
      "max_tokens": 128
    },
    {
      "name": "Mistral (Completo)", 
      "description": "Modelo completo para tarefas complexas (4GB)",
      "max_tokens": 256
    }
  ]
}
```

---

## üí° Exemplos Pr√°ticos

### Teste R√°pido (TinyLlama)
```bash
curl -X POST http://localhost/api/generate \
  -H 'Content-Type: application/json' \
  -d '{"prompt": "nmap scan b√°sico", "technique": "recon"}'
```
**Resultado:** Resposta em ~5-10 segundos ‚ö°

### An√°lise Complexa (Mistral)
```bash
curl -X POST http://localhost/api/generate \
  -H 'Content-Type: application/json' \
  -d '{"prompt": "Desenvolver exploit buffer overflow avan√ßado", "technique": "advanced_exploit"}'
```
**Resultado:** An√°lise detalhada (~30 segundos, alta qualidade)

### Interface Web - Chat Flutuante
1. Acesse http://localhost
2. Clique no bot√£o de chat (canto inferior direito)
3. Digite: *"Gere reverse shell para Linux"*
4. **Resultado:** TinyLlama responde instantaneamente! ‚ö°

---

## üîß Configura√ß√µes Avan√ßadas

### For√ßar Modelo Espec√≠fico
```bash
# Sempre usar TinyLlama (ultra-r√°pido)
curl -X POST http://localhost/api/generate \
  -d '{"prompt": "...", "technique": "recon"}'

# Usar Mistral (mais lento, melhor qualidade)
curl -X POST http://localhost/api/generate \
  -d '{"prompt": "...", "technique": "complex_analysis"}'
```

### Otimizar para M√°quina Limitada
1. Use apenas TinyLlama (remova/renomeie o arquivo Mistral)
2. Sistema detecta automaticamente e usa s√≥ o modelo r√°pido
3. **Benef√≠cio:** Todas as respostas em ~5-10 segundos

---

## ‚ùó Solu√ß√£o de Problemas

### Modelo Indispon√≠vel (503)
```bash
# Verificar se modelos existem
ls -la models/

# Re-baixar TinyLlama se necess√°rio
cd models
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
```

### Respostas Muito Lentas
- **Causa:** Tentando usar Mistral em m√°quina limitada
- **Solu√ß√£o:** Remova o arquivo `mistral-7b-instruct-v0.2.Q4_K_M.gguf` para for√ßar apenas TinyLlama

### Interface N√£o Carrega
- **Causa:** Conflito de CORS/portas
- **Solu√ß√£o:** Use Docker (`http://localhost`) em vez de acesso direto aos arquivos
