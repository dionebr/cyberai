# AceleraÃ§Ã£o GPU para CyberAI

## ğŸ® VisÃ£o Geral

O CyberAI agora suporta aceleraÃ§Ã£o GPU NVIDIA para aumentar significativamente a velocidade de geraÃ§Ã£o de respostas.

## ğŸ“‹ PrÃ©-requisitos

âœ… **JÃ¡ configurado no seu sistema:**
- GPU NVIDIA GTX 1650 (4GB VRAM) 
- Driver NVIDIA 580.82.09
- CUDA 13.0 suportado
- nvidia-container-toolkit instalado

## âš¡ BenefÃ­cios de Performance

| Modelo | CPU (atual) | GPU (novo) | Melhoria |
|--------|-------------|------------|----------|
| TinyLlama 1.1B | ~8-10s | ~3-4s | 2-3x mais rÃ¡pido |
| Mistral 7B | ~15-20s | ~4-6s | 4-5x mais rÃ¡pido |

## ğŸš€ Como Usar

### Modo GPU (Recomendado)
```bash
# Iniciar com aceleraÃ§Ã£o GPU
docker-compose -f docker-compose.gpu.yml up --build

# Para parar
docker-compose -f docker-compose.gpu.yml down
```

### Modo CPU (PadrÃ£o)
```bash
# Iniciar sem GPU
docker-compose up

# Para parar  
docker-compose down
```

## ğŸ”§ ConfiguraÃ§Ã£o AutomÃ¡tica

O sistema detecta automaticamente se hÃ¡ suporte GPU:
- âœ… **Com GPU**: Usa CUDA automaticamente
- âš™ï¸ **Sem GPU**: Funciona normalmente na CPU

## ğŸ“Š Monitoramento GPU

Verifique o uso da GPU durante inferÃªncia:
```bash
# Ver status da GPU
nvidia-smi

# Monitoramento em tempo real
watch -n 1 nvidia-smi
```

## ğŸ› ï¸ SoluÃ§Ã£o de Problemas

### GPU nÃ£o detectada
```bash
# Verificar drivers
nvidia-smi

# Reinstalar container toolkit se necessÃ¡rio
sudo pacman -S nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Erro de memÃ³ria VRAM
A GTX 1650 (4GB) pode carregar:
- âœ… TinyLlama 1.1B (600MB) + sistema
- âœ… Mistral 7B Q4 (4GB) - uso total de VRAM
- âŒ Modelos maiores (>4GB) - usar CPU

### Build lento na primeira vez
O primeiro build baixa ~3GB de imagem CUDA base:
- â±ï¸ Tempo estimado: 10-15 min (depende da internet)
- ğŸ’¾ EspaÃ§o necessÃ¡rio: ~6GB adicional
- ğŸ”„ PrÃ³ximos builds sÃ£o mais rÃ¡pidos (cache)

## ğŸ¯ ConfiguraÃ§Ãµes Otimizadas

O sistema usa configuraÃ§Ãµes otimizadas automaticamente:

**TinyLlama GPU:**
- 35 camadas na GPU (todas)
- Batch size: 512
- Context: 768 tokens

**Mistral 7B GPU:**  
- 32 camadas na GPU (mÃ¡ximo)
- Batch size: 512
- Context: 2048 tokens

## ğŸ“ˆ PrÃ³ximos Passos

1. **Modelos maiores**: Quando houver GPU com mais VRAM
2. **QuantizaÃ§Ã£o**: Modelos INT4/INT8 para melhor velocidade
3. **Multi-GPU**: Suporte para mÃºltiplas GPUs
4. **AMD GPU**: Suporte ROCm para GPUs AMD