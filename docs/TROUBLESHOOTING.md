# Solu√ß√£o de Problemas - CyberAI

## üîß Problemas Comuns do Sistema H√≠brido

### ‚ö° Modelo TinyLlama N√£o Encontrado
**Erro:** `Modelo GGUF n√£o encontrado em [...]/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf`

**Solu√ß√µes:**
```bash
# M√©todo 1: Download direto
cd models
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf

# M√©todo 2: Via huggingface-cli
huggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf --local-dir models --local-dir-use-symlinks False

# Verificar se foi baixado
ls -la models/*.gguf
```

### üêå Respostas Muito Lentas (>30s)
**Causa:** Sistema tentando usar Mistral em m√°quina limitada

**Solu√ß√µes:**
```bash
# Op√ß√£o 1: Remover Mistral (for√ßar apenas TinyLlama)
cd models
mv mistral-7b-instruct-v0.2.Q4_K_M.gguf mistral-7b-instruct-v0.2.Q4_K_M.gguf.backup

# Op√ß√£o 2: Verificar qual modelo est√° sendo usado
curl http://localhost/api/models

# Op√ß√£o 3: Usar apenas t√©cnicas r√°pidas (TinyLlama)
# recon, payloads, xss, sql_injection, buffer_overflow, etc.
```

### üîÑ Sistema N√£o Seleciona Modelo Correto
**Problema:** Sempre usa o mesmo modelo

**Diagn√≥stico:**
```bash
# Verificar modelos dispon√≠veis
curl http://localhost/api/models

# Teste com t√©cnica r√°pida (deve usar TinyLlama)
curl -X POST http://localhost/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"nmap scan","technique":"recon","max_tokens":50}'

# Verificar resposta - deve conter: "model_used":"Modelo r√°pido para tarefas simples"
```

---

## üê≥ Problemas Docker

### Container API N√£o Inicia
**Erro:** `Container cyberai-api-1 exited with code 1`

**Diagn√≥stico:**
```bash
# Ver logs detalhados
docker-compose logs api

# Poss√≠veis causas e solu√ß√µes:
# 1. Modelo n√£o encontrado
docker exec -it cyberai-api-1 ls -la /app/models/

# 2. Depend√™ncias n√£o instaladas
docker-compose build --no-cache api

# 3. Porta ocupada
fuser -k 8000/tcp
docker-compose restart api
```

### Erro de Permiss√£o nos Modelos
**Erro:** `Permission denied` ao acessar modelo

**Solu√ß√£o:**
```bash
# Corrigir permiss√µes
sudo chown -R $USER:$USER models/
chmod 644 models/*.gguf

# Recriar containers
docker-compose down
docker-compose up --build -d
```

---

## üíª Problemas Instala√ß√£o Local

### llama-cpp-python N√£o Funciona
**Erro:** `ImportError: cannot import name 'Llama'`

**Solu√ß√µes por Plataforma:**
```bash
# Linux CPU (gen√©rico)
pip install llama-cpp-python

# Linux com NVIDIA GPU
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python

# Linux com AMD GPU
CMAKE_ARGS="-DLLAMA_HIPBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python

# macOS Apple Silicon
CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python

# Windows
# Use: pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
```

### Ambiente Virtual N√£o Funciona
**Problema:** Depend√™ncias n√£o encontradas

**Solu√ß√£o Completa:**
```bash
# Remover ambiente existente
rm -rf .venv

# Criar novo ambiente
python3 -m venv .venv
source .venv/bin/activate

# Atualizar pip
pip install --upgrade pip

# Instalar depend√™ncias na ordem correta
pip install wheel setuptools
pip install -r requirements.txt

# Verificar instala√ß√£o
python -c "from llama_cpp import Llama; print('OK')"
```

---

## üåê Problemas Interface Web

### API 404 Not Found
**Erro:** `GET http://localhost/api/techniques 404 (Not Found)`

**Verifica√ß√µes:**
```bash
# 1. Verificar se nginx est√° proxy correto
docker exec -it cyberai-web-1 cat /etc/nginx/conf.d/default.conf

# 2. Testar API diretamente
curl http://localhost:8000/api/techniques

# 3. Verificar containers rodando
docker-compose ps

# 4. Reiniciar nginx
docker-compose restart web
```

### JavaScript Errors
**Erro:** `Uncaught ReferenceError: detectApiBase is not defined`

**Solu√ß√£o:**
```bash
# Verificar se utils.js est√° carregando
curl http://localhost/static/js/utils.js

# Verificar no browser DevTools:
# F12 ‚Üí Console ‚Üí deve mostrar erros espec√≠ficos

# Se necess√°rio, recriar containers
docker-compose down
docker-compose up --build -d
```

### Timeout nas Requisi√ß√µes Frontend
**Erro:** Request timeout ap√≥s 30 segundos

**Solu√ß√µes:**
1. **Para m√°quinas limitadas:**
   ```bash
   # Remover Mistral, manter apenas TinyLlama
   cd models && mv mistral*.gguf mistral.backup
   docker-compose restart api
   ```

2. **Aumentar timeout no frontend:**
   ```javascript
   // Em cada p√°gina HTML, aumentar max-time
   fetch(url, { signal: AbortSignal.timeout(60000) }) // 60s
   ```

---

## ‚ö° Otimiza√ß√µes de Performance

### M√°quina com Pouca RAM (<4GB)
```bash
# Usar apenas TinyLlama
cd models
mkdir backup && mv mistral*.gguf backup/

# Verificar uso apenas do modelo r√°pido
curl http://localhost/api/models
# Deve mostrar apenas TinyLlama
```

### M√°quina com Boa Performance (>8GB RAM)
```bash
# Manter ambos os modelos para sistema h√≠brido
ls models/
# Deve ter: tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf E mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Verificar sele√ß√£o autom√°tica
curl -X POST http://localhost/api/generate -H "Content-Type: application/json" -d '{"prompt":"teste","technique":"recon"}'
# Deve usar TinyLlama (~5s)

curl -X POST http://localhost/api/generate -H "Content-Type: application/json" -d '{"prompt":"an√°lise complexa","technique":"advanced_analysis"}'  
# Deve usar Mistral (~30s)
```

---

## üîç Ferramentas de Diagn√≥stico

### Script de Diagn√≥stico R√°pido
```bash
#!/bin/bash
echo "=== CyberAI System Check ==="
echo "1. Modelos dispon√≠veis:"
ls -la models/*.gguf

echo "2. API Status:"
curl -s http://localhost:8000/api/models || echo "API n√£o responde"

echo "3. Containers Docker:"
docker-compose ps

echo "4. Logs recentes da API:"
docker-compose logs --tail=5 api

echo "5. Teste r√°pido TinyLlama:"
curl -s -X POST http://localhost/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"test","technique":"recon","max_tokens":10}' \
  | grep -o '"model_used":"[^"]*"'
```

### Verifica√ß√£o Manual Completa
```bash
# 1. Verificar modelos
ls -la models/
# Esperado: 2 arquivos .gguf (tinyllama + mistral) OU apenas tinyllama

# 2. Testar API
curl http://localhost/api/models
# Esperado: JSON com informa√ß√µes dos modelos

# 3. Teste funcional
curl -X POST http://localhost/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"nmap","technique":"recon","max_tokens":20}'
# Esperado: Resposta em ~5-10 segundos

# 4. Verificar frontend
curl http://localhost/
# Esperado: HTML da p√°gina inicial
```

---

## üÜò Suporte e Recursos

### Logs Importantes
```bash
# API logs (Docker)
docker-compose logs -f api

# Web logs (Docker)  
docker-compose logs -f web

# Sistema local
tail -f ~/.local/share/cyberai/logs/api.log
```

### Restaurar Estado Limpo
```bash
# Parar tudo
docker-compose down

# Limpar containers e imagens
docker system prune -a

# Recriar do zero
git pull origin main
docker-compose up --build -d
```

### Community & Issues
- **GitHub Issues:** https://github.com/dionebr/cyberai/issues
- **Documenta√ß√£o:** https://github.com/dionebr/cyberai/tree/main/docs
- **Discord:** [Em breve]
