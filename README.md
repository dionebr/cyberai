# CyberAI: Assistente Ofensivo de Ciberseguran√ßa ‚ö°

**CyberAI** √© uma su√≠te de ferramentas acelerada por **sistema h√≠brido de LLMs** (TinyLlama + Mistral), projetada para apoiar profissionais e entusiastas de ciberseguran√ßa em tarefas ofensivas. A plataforma opera de forma **100% offline**, garantindo privacidade e seguran√ßa, e serve como um poderoso assistente para treinamento, pentesting e pesquisa em ambientes como HackTheBox, TryHackMe e VulnHub.

üöÄ **Novo:** Sistema h√≠brido inteligente que seleciona automaticamente o modelo mais adequado (TinyLlama para velocidade ou Mistral para complexidade), oferecendo **respostas at√© 20x mais r√°pidas** para tarefas simples!

![CyberAI Screenshot](https://raw.githubusercontent.com/dionebr/cyberai/main/docs/images/cyberai-showcase.png)

---

## ‚ú® Funcionalidades Principais

-   üß† **Sistema H√≠brido Inteligente:** Dois modelos LLM trabalhando juntos:
    -   **TinyLlama 1.1B (668MB):** Respostas ultrarr√°pidas para tarefas simples
    -   **Mistral 7B (4GB):** Alta qualidade para an√°lises complexas
-   ‚ö° **Performance Otimizada:** Sele√ß√£o autom√°tica do modelo baseada na complexidade da tarefa
-   üîí **100% Offline:** Utilize modelos de linguagem de ponta (GGUF) sem depender de APIs externas
-   üñ•Ô∏è **Interface Web Intuitiva:** Acesse todas as ferramentas atrav√©s de uma interface moderna e responsiva
-   üéØ **Su√≠te de M√≥dulos Especializados:** Cobertura completa do ciclo de pentesting, desde o reconhecimento at√© a p√≥s-explora√ß√£o
-   üíæ **Economia de Recursos:** Ideal para m√°quinas limitadas - usa apenas 668MB para tarefas comuns
-   üê≥ **Ambiente Dockerizado:** Instala√ß√£o e execu√ß√£o simplificadas com Docker e Nginx
-   üîê **Privacidade Total:** Seus dados e intera√ß√µes nunca saem da sua m√°quina local

---

## üß† Sistema H√≠brido de IA

O CyberAI utiliza um **sistema h√≠brido inovador** que combina dois modelos de IA para otimizar performance e qualidade:

### ‚ö° TinyLlama 1.1B (Modelo R√°pido)
- **Tamanho:** 668MB
- **Velocidade:** Respostas em 5-10 segundos  
- **Uso:** Tarefas simples e r√°pidas
- **T√©cnicas:** recon, payloads, exploits b√°sicos, relat√≥rios, XSS, SQL injection, etc.

### üéØ Mistral 7B (Modelo Completo) 
- **Tamanho:** 4GB
- **Qualidade:** An√°lises detalhadas e complexas
- **Uso:** Tarefas que exigem racioc√≠nio avan√ßado
- **T√©cnicas:** An√°lises de malware complexas, desenvolvimento de exploits avan√ßados

### üîÑ Sele√ß√£o Autom√°tica Inteligente
O sistema escolhe automaticamente o modelo mais adequado com base na t√©cnica solicitada, garantindo:
- ‚ö° **Velocidade m√°xima** para tarefas comuns
- üéØ **Qualidade premium** quando necess√°rio  
- üíæ **Economia de recursos** computacionais
- üì± **Compatibilidade** com m√°quinas limitadas

---

## üöÄ M√≥dulos Dispon√≠veis

A plataforma √© organizada em 12 m√≥dulos, cada um focado em uma √°rea espec√≠fica do pentesting:

| M√≥dulo | Cor | Descri√ß√£o |
| :--- | :--- | :--- |
| **Reconhecimento** | üü¢ | Enumera√ß√£o de alvos, varredura de portas, descoberta de subdom√≠nios e tecnologias. |
| **Payloads** | üîµ | Gera√ß√£o de reverse shells, stagers e ofusca√ß√£o para evas√£o de defesas. |
| **An√°lise de Bin√°rios**| üü£ | Engenharia reversa, an√°lise de malware e extra√ß√£o de IoCs. |
| **Explora√ß√£o** | üî¥ | Gera√ß√£o de provas de conceito (PoCs) para CVEs e vulnerabilidades conhecidas. |
| **P√≥s-Explora√ß√£o** | üü° | T√©cnicas de persist√™ncia, movimento lateral e coleta de credenciais. |
| **Treinamento** | üå∏ | Cria√ß√£o de trilhas de estudo com CTFs, laborat√≥rios e guias pr√°ticos. |
| **Relat√≥rios** | üíß | Estrutura√ß√£o de relat√≥rios t√©cnicos, desde o sum√°rio executivo at√© as recomenda√ß√µes. |
| **Gest√£o** | üü† | Planejamento de engajamentos, defini√ß√£o de escopo, alvos e m√©tricas. |
| **Dev Tools** | üçã | Assist√™ncia para desenvolvimento de ferramentas, com templates e snippets seguros. |
| **Threat Intel** | üçµ | Briefings sobre amea√ßas, mapeamento de TTPs no MITRE ATT&CK e an√°lise de CVEs. |
| **Laborat√≥rio** | üéÄ | Guias para montagem de ambientes de teste seguros e isolados. |
| **Comunidade** | ‚ö´ | Central para plugins, integra√ß√µes e recursos compartilhados. |

---

## üê≥ Instala√ß√£o e Uso (Docker - Recomendado)

Este √© o m√©todo mais simples e r√°pido para ter o ambiente completo e funcional.

### Requisitos
-   [Docker](https://docs.docker.com/get-docker/) e [Docker Compose](https://docs.docker.com/compose/install/)
-   `git`
-   Conex√£o com a internet (apenas para o primeiro build e download do modelo)

### Passos

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/dionebr/cyberai.git
    cd cyberai
    ```

2.  **Construa e inicie os servi√ßos:**
    O comando a seguir ir√° construir a imagem da API, baixar o Nginx e iniciar ambos os cont√™ineres em background.
    ```bash
    docker compose up --build -d
    ```

3.  **Acesse a plataforma:**
    Abra seu navegador e acesse:
    -   **URL:** `http://localhost`

4.  **(Opcional) Acompanhe os logs:**
    Para ver os logs da API e do Nginx em tempo real:
    ```bash
    docker compose logs -f
    ```

---

## üíª Instala√ß√£o e Uso (Local - Alternativo)

Para usu√°rios que preferem rodar os servi√ßos diretamente no host.

### Requisitos
-   Python 3.10+
-   `git`
-   `npm` (Node.js) para o build do CSS

### Passos

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/dionebr/cyberai.git
    cd cyberai
    ```

2.  **Execute o script de instala√ß√£o:**
    Este script ir√° criar um ambiente virtual, instalar as depend√™ncias Python e Node, e compilar o CSS.
    ```bash
    bash src/scripts/install.sh
    ```
    *Nota: O script de download do modelo pode ser executado separadamente, se necess√°rio.*

3.  **Baixe os modelos LLM:**
    
    **TinyLlama (Essencial - 668MB):**
    ```bash
    cd models
    wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
    ```
    
    **Mistral (Opcional - 4GB):**
    ```bash
    huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir models --local-dir-use-symlinks False
    ```
    
    > üí° **Dica:** Para m√°quinas limitadas, use apenas TinyLlama. Para m√°xima qualidade, baixe ambos os modelos.

4.  **Inicie os servi√ßos:**
    Voc√™ precisar√° de dois terminais.

    -   **Terminal 1: Iniciar a API**
        ```bash
        # Ative o ambiente virtual
        source .venv/bin/activate
        
        # Inicie o servidor da API
        python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000
        ```

    -   **Terminal 2: Iniciar o Servidor Web**
        ```bash
        # O script 'npm start' serve a pasta 'src/web' corretamente
        npm start
        ```

5.  **Acesse a plataforma:**
    Abra seu navegador e acesse:
    -   **URL:** `http://localhost:8080/templates/index.html`

---

## üõ†Ô∏è Solu√ß√£o de Problemas (Troubleshooting)

-   **Erro 503 `Modelo indispon√≠vel` na API:**
    -   **Causa:** O arquivo do modelo GGUF n√£o foi encontrado.
    -   **Solu√ß√£o:**
        ```bash
        # Baixar TinyLlama (essencial)
        cd models
        wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
        
        # Verificar se foi baixado
        ls -la models/*.gguf
        
        # Reiniciar containers
        docker-compose restart api
        ```

-   **Respostas muito lentas (>30s):**
    -   **Causa:** Tentando usar Mistral em m√°quina limitada.
    -   **Solu√ß√£o:** Use apenas TinyLlama removendo o arquivo Mistral:
        ```bash
        cd models && mv mistral*.gguf mistral.backup
        docker-compose restart api
        ```

-   **Sistema n√£o seleciona modelo correto:**
    -   **Verifica√ß√£o:** 
        ```bash
        curl http://localhost/api/models
        # Deve mostrar TinyLlama para t√©cnicas r√°pidas
        ```

-   **Erro 404 em `/api/techniques`:**
    -   **Causa:** Proxy nginx n√£o configurado corretamente.
    -   **Solu√ß√£o:** Use Docker compose (recomendado) ou verifique configura√ß√£o nginx.

---

### üí° Exemplos de Uso

### Chat Interativo ‚ö°
Use o chat flutuante na p√°gina inicial para perguntas r√°pidas com **TinyLlama**.
> **Voc√™:** "Gere um comando Nmap agressivo para o alvo 10.10.11.123"  
> **CyberAI:** Resposta instant√¢nea em ~5 segundos!

### M√≥dulo de Payloads üöÄ  
Acesse o m√≥dulo "Payloads" e preencha os campos para gerar reverse shells customizadas.
> **IP:** `10.10.14.2`, **Porta:** `9001`, **Plataforma:** `Linux`  
> **Resultado:** M√∫ltiplas varia√ß√µes de reverse shells em segundos

### Sistema H√≠brido em A√ß√£o üß†
```bash
# Consulta r√°pida (TinyLlama - 5s)
curl -X POST http://localhost/api/generate \
-H "Content-Type: application/json" \
-d '{"prompt": "comandos de enumera√ß√£o SMB", "technique": "recon"}'

# An√°lise complexa (Mistral - 30s, alta qualidade)  
curl -X POST http://localhost/api/generate \
-H "Content-Type: application/json" \
-d '{"prompt": "desenvolver exploit ROP chain para bypass ASLR", "technique": "advanced_exploit"}'
```

### Verificar Modelos Ativos üìä
```bash
curl http://localhost/api/models
# Mostra quais modelos est√£o dispon√≠veis e suas especialidades
```

---

## üéØ Performance Comparison

| Cen√°rio | Modelo | Tempo | Qualidade | Uso Recomendado |
|---------|--------|--------|-----------|-----------------|
| Comandos b√°sicos | TinyLlama | ~5s | ‚≠ê‚≠ê‚≠ê‚≠ê | Tarefas di√°rias |
| Enumera√ß√£o/Recon | TinyLlama | ~5-10s | ‚≠ê‚≠ê‚≠ê‚≠ê | HTB, THM, CTFs |
| Web exploits | TinyLlama | ~5-10s | ‚≠ê‚≠ê‚≠ê‚≠ê | OWASP Top 10 |  
| Payloads/Scripts | TinyLlama | ~5-10s | ‚≠ê‚≠ê‚≠ê‚≠ê | Red team, Pentest |
| An√°lise profunda | Mistral | ~20-30s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Research, 0days |
| Malware analysis | Mistral | ~20-30s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Forensics, IR |

---

## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir uma *issue* para relatar bugs ou sugerir novas funcionalidades. 

### üöÄ Roadmap
- [ ] Integra√ß√£o com mais modelos (CodeLlama, Llama3, etc.)
- [ ] Sistema de caching inteligente
- [ ] Interface mobile otimizada
- [ ] Plugin system para extens√µes
- [ ] Integra√ß√£o com ferramentas populares (Burp, Metasploit)

### üí° Como Contribuir
1. Fork o reposit√≥rio
2. Crie sua feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ‚≠ê Caracter√≠sticas T√©cnicas

### üß† Sistema H√≠brido de IA
- **TinyLlama 1.1B:** 668MB, ~5-10s por resposta
- **Mistral 7B:** 4GB, ~20-30s por resposta
- **Sele√ß√£o Autom√°tica:** Baseada em complexidade da t√©cnica
- **Fallback Inteligente:** Se um modelo falha, tenta o outro

### ‚ö° Otimiza√ß√µes de Performance  
- **Quantiza√ß√£o Q4_K_M:** Melhor balan√ßo qualidade/performance
- **Context Length:** Otimizado por modelo (512 vs 1024 tokens)
- **Batch Processing:** Ajustado por arquitetura (64 vs 32)
- **Threading:** CPU cores otimizados por modelo

### üîí Seguran√ßa e Privacidade
- **100% Offline:** Nenhum dado sai da sua m√°quina
- **Sem Telemetria:** Zero coleta de dados
- **Modelos Locais:** Controle total sobre IA
- **Containerizado:** Isolamento completo via Docker

## üìä Especifica√ß√µes M√≠nimas vs Recomendadas

| Componente | M√≠nimo (TinyLlama) | Recomendado (H√≠brido) |
|------------|--------------------|-----------------------|
| **RAM** | 2GB | 8GB+ |
| **Storage** | 2GB | 10GB |  
| **CPU** | 2 cores | 4+ cores |
| **GPU** | Nenhuma | CUDA/OpenCL (opcional) |
| **OS** | Linux/macOS/Windows | Linux (preferencialmente) |

## üìú Licen√ßa

Este projeto √© distribu√≠do sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

---

### üéØ **Experimente agora:**
```bash
git clone https://github.com/dionebr/cyberai.git
cd cyberai  
docker compose up --build -d
# Acesse: http://localhost
```

**CyberAI** - *Onde velocidade encontra intelig√™ncia em ciberseguran√ßa* üõ°Ô∏è‚ö°